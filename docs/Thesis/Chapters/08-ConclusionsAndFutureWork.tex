\chapter{Conclusions and Future Work}
\label{Chapter-Conclusions-and-Future-Work}

\section{Conclusions}
This study conducted an in-depth analysis of the behaviour of an FL system under training. It revealed that the batch size is its most important parameter, as it significantly impacts most characteristics of the training, including its convergence rate and the local training time. In more detail, it showed that, depending on data distribution, there is only a small range of small batch sizes where the algorithm can provide an acceptable solution in a reasonable time.

The vast re-configuration abilities provided by FPGAs can be exploited to create an accelerator that specifically targets that narrow range. Other technologies, like the GPU, can show superior overall throughput, but they are ineffectual if they cannot provide it with the right batch sizes. While flawed, as explained in section \ref{Constrains}, the implemented architecture managed to outperform its comparisons with both IID and non-IID data distributions.

Last but not least, the benchmarks presented in chapter \ref{Chapter-Results} may make the choice of the underlying platform appear inconsequential. It is quite common in on-edge settings that responsible for most of the training latency is the communication between server and clients. However, in chapter \ref{Chapter-Robustness-Analysis}, it was demonstrated that this is not always the case. Therefore, we can conclude that FPGAs are more than appropriate for accelerating Federated Learning.

\section{Future Work}
This work sheds light on how FL and FPGA-based architectures interact with each other and cooperate. However, there is still room for improvement and additional synergies are available to be explored and exploited.

\subsection{Quantization}
The majority of studies on quantization in FL have focused on communication compression, e.g. \cite{https://doi.org/10.48550/arxiv.2006.10672}, and recent works have tried to expand it to local training \cite{https://doi.org/10.48550/arxiv.2206.10844}. FPGAs have been proven that they can provide efficient and robust implementations of quantized ANNs \cite{Bacchus2020}. In our opinion, implementing FL with quantized ANNs or ANNs robust to quantization on FPGA accelerators, is the most promising area to expand this work.

\subsection{Encryption \& Privacy}
FL systems often utilize homomorphic encryption to improve security and privacy. It has been demonstrated that it can be efficiently implemented in FPGAs \cite{FPGA_encryption_for_FL}. Attaching it to an FPGA-design that accelerates training, like the one developed in this work, can present interesting challenges such resource management, efficient pipelinining between the two components etc.

Techniques like differential privacy are frequently used to increase the privacy of the participating clients \cite{Wei2020}, with the cost of extra computation. These methods typically entail element-wise transformations on the generated local models, such as adding random Gaussian noise to each variable and clipping those that exceed a predetermined threshold. As there are no inner dependencies, an FPGA implementation can completely parallelize them and virtually eliminate their latency.

\subsection{Platforms}
The technology dissimilarities between the benchmarked platforms should be noted. The CPU and GPU, while consumer products, were released four years later after the FPGA. To have a more thorough understanding of the system, it is important to test with other platforms, in addition to different technologies such as TPUs.

Regarding the design on the FPGA, a number of improvements can be made. Implementing it with RTL, completely or just certain of its hardware functions, can greatly increase its performance by avoiding the inefficiencies and bugs of the HLS tools.

\subsection{Scale}
This work mostly experimented with around 2 to 20 clients. It is highly likely that FL will behave differently in scenarios involving hundreds or thousands of clients. Furthermore, compared to the CPU and GPU, the FPGA-based design performed relatively better with small datasets and batches. Hence, exploring FPGA-based solutions in FL settings with a large number of clients is an intriguing direction for future development.

\subsection{Models}
As shown in chapter \ref{Chapter-Robustness-Analysis}, the behaviour of the FL algorithm heavily depends on the size and architecture of the model being trained. Furthermore, the model implemented in this work, can not be called representative of all ANNs.  Thus, more experiments should be conducted, with ANNs of different sizes and types.