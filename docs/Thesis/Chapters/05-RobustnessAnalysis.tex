\chapter{Robustness Analysis}
\label{Chapter-Robustness-Analysis}

A number of experiments, each concentrating on a different component of the FL algorithm, have been conducted to demonstrate the robustness of the developed FL environment. Additionally, the tests begin with straightforward cases and progress to more complicated ones by building on their findings. The training problem is image recognition on the Fashion-MNIST dataset. % why the experiments, from simple cases to more complex ones, problem 

As the main goal of these experiments is to prove the algorithmic soundness of the FL environment, they were carried out on a single machine. As a result, the participating processes are competing for computing resources, and communication takes place on the operating system's loopback. Thus, it is impossible to draw any meaningful real-time inferences from these experiments; instead the communication frequency is used as a benchmark value. % what looking into and why.

\section{Distributed SGD with IID data}
The first experiment focuses on the most straitforward case, distributed SGD with IID data. The training dataset is split equally between the participating clients. The third model in the collection is used, and for simplicity's sake, the Adam optimizer with default parameters is employed. 

\begin{table}[H]
    \center
    \begin{tabular}{ | c | c | }
        \hline
        \multicolumn{2}{ | c | }{ parameters } \\
        \hline\hline
        participating clients & 4 \\
        \hline
        local epochs & 1 \\
        \hline
        steps per epoch & 3 \\
        \hline
        batch size & 10 \\
        \hline
    \end{tabular}
    \caption[Experiment 1 Parameters]{Parameters of the first experiment.}
    \label{table:Experiment 1 parameters}
\end{table}

Using the aforementioned parameters, each client consumes 30 examples per GE. Considering that all four clients participate in each GE, 500 GEs are necessary to exhaust all training data.

The FL trained model is compared to a centrally trained one with same parameters. To do this properly, a common scale is required. As such, the number of times the training dataset is repeated is used.

\begin{figure}[H]
    \center
    \input{experiments/1.tex}
    \caption[Experiment 1 results]{Experiment 1 results}
    \label{fig:Experiment 1 results}
\end{figure}

The aforementioned findings demonstrate that training the model via FL yields the same accuracy as training it centrally, albeit at a slower rate. This is understandable given that the model in the first case is updated every 150 examples, whereas in the second case it is updated every 10 examples.

Another observation is the re-balancing effect of the FL algorithm. In centralized training, due to overfitting, the accuracy of the model degrades after peaking. This is not true when trained under the federated setting, as overfitted parameters are regularized when averaging multiple local models.

\section{Distributed SGD with non-IID data}
In this experiment, the third model is trained with distributed SGD and a pathological non-IID dataset. It is interesting to see how the batch size affects the performance of Distributed SGD, given that it is notorious for being unable to handle non-IID datasets. Thus, the test was repeated with three distinct combinations of parameters.

\begin{table}[H]
    \center
    \begin{tabular}{ | c | c | c | c | }
        \hline
        case & 1 & 2 & 3\\
        \hline\hline
        participating clients & 5 & 5 & 5 \\
        \hline
        local epochs & 1 & 1 & 1 \\
        \hline
        steps per epoch & 1 & 1 & 1 \\
        \hline
        batch size & 1 & 2 & 4 \\
        \hline
    \end{tabular}
    \caption[Experiment 2 Parameters]{Parameters of the second experiment.}
    \label{table:Experiment 2 parameters}
\end{table}

The dataset is split between 5 clients, with each one getting all the examples of two labels. The first client holds all the examples with labels 0 or 1, the second client holds all the examples with labels 2 or 3 etc. As clients holds no knowledge on the other classes, self-training the model can only achieve a maximum accuracy of 20\%. Therefore, it is required to either centralize the dataset or use a decentralized training method.

\begin{figure}[H]
    \center
    \input{experiments/2.tex}
    \caption[Experiment 2 results]{Experiment 2 results}
    \label{fig:Experiment 2 results}
\end{figure}

Distributed SGD appears to struggle with non-IID data. With a batch size of just one example, it achieves accuracy consistent with prior works\cite{FL-original-paper}, but it is unable to converge with bigger batch sizes. This observation is consistent with FL theory, and in order to improve outcomes, additional techniques such as data rebalancing or expanding the client pool are needed.

\section{Client Selection}
In FL, it is frequently preferable to use a portion of the clients in each GE when there are several of them. In this method, data efficiency and model performance are improved since the global model can be updated more times before the training data run out. This experiment aims to test this functionality.

Eight clients are participating in training the Lenet-5 model. Every GE, only three clients are used. The dataset is split into 8 identically sized, mutually exclusive random shards, each of which is given to a client.

\begin{table}[H]
    \center
    \begin{tabular}{ | c | c | }
        \hline
        \multicolumn{2}{|c|}{ parameters } \\
        \hline\hline
         total clients & 8 \\
        \hline
        clients per GE & 3 \\
        \hline
        local epochs & 1 \\
        \hline
        steps per epoch & 2 \\
        \hline
        batch size & 20 \\
        \hline
    \end{tabular}
    \caption[Experiment 3 Parameters]{The parameters of the third experiment.}
    \label{table:Experiment 3 parameters}
\end{table}

Data reshuffling is also incorporated in FL and centralized training. When all of the examples of a dataset have been used, it is resuffled and rebatched. Overfitting is thereby expected to diminish in both scenarios.

\begin{figure}[H]
    \center
    \input{experiments/3.tex}
    \caption[Experiment 3 results]{Experiment 3 results}
    \label{fig:Experiment 3 results}
\end{figure}

In comparison to the first trial, where there was no client selection, FL training produces results that are comparable to those of centralized training more quickly. Furthermore, overfitting is decreased in both scenarios.

\section{Greater data per GE consumption}
The primary objective of this experiment is to assess the impact of increasing the consumption of local data per GE, prior migrating to the Federated Averaging algorithm. Furthermore, the FL environment is tested with a more complex architecture by using the ninth model that contains an inception module and a dropout layer.

The data is distributed randomly to 5 clients, but only 3 of them are used each GE. Two sets of parameters are used, with different number of local updates per GE.
    
\begin{table}[H]
    \center
    \begin{tabular}
        { | l | c | c | c | }
        \hline
        parameters & FL set 1 & FL set 2 & Centralized training\\\hline
        total clients & 5 & 5 & 1\\\hline
        clients per GE & 3 & 3 & 1\\\hline
        steps per GE & 1 & 2 & examples/batch size \\\hline
        batch size & 20  & 20  & 20 \\\hline
        examples per GE & 60  & 120  & all \\\hline
        GEs to use all examples & 1000  & 500  & - \\\hline
    \end{tabular}
    \caption[Experiment 4 parameters]{Experiment 4 parameters}
    \label{table:Experiment 4 parameters}
\end{table}

It is important to note that, compared to the first set of parameters, the second one needs only half as many communication rounds to exhaust the dataset.
    
\begin{figure}[H]
    \center
    \input{experiments/4.tex}
    \caption[Experiment 4 results]{Experiment 4 results}
    \label{fig:Experiment 4 results}
\end{figure}

Both FL scenarios reach comparable accuracy with centralized training. Although the second one appears to progress at a slower pace than the first, it only updates the global model half as often and needs half as much communication. This results in double the computation to communication ratio and being a more viable target for parallelization.

\section{Client Fault Tolerance}
In an edge environment, the clients may be unreliable and any algorithm must be resilient to random faults. This experiment aims to simulate such a case. To achieve this, 6 clients are initially participating in training the third model, but around 1/10 into training one of them abruptly disconnects. That means for 90\% of the training, 1/6 of the data are inaccessible.

\begin{table}[H]
    \center
    \begin{tabular}
        { | l | c | c | }
        \hline
        parameters & normal op & faulty op\\\hline
        total clients   & 5 & 6\\\hline
        clients per GE  & 3 & 3\\\hline
        steps per GE    & 1 & 1\\\hline
        batch size      & 20 & 20\\\hline
    \end{tabular}
    \caption[Experiment 5 parameters]{Experiment 5 parameters}
    \label{table:Experiment 5 parameters}
\end{table}
    
\begin{figure}[H]
    \center
    \input{experiments/5.tex}
    \caption[Experiment 5 results]{Experiment 5 results}
    \label{fig:Experiment 5 results}
\end{figure}

Although the model's final accuracy drops, the effect is manageable as training continues and accuracy is still within acceptable bounds. In a real-world scenario, this issue can be resolved by postponing a portion of the training until after lost data resurfaces or new data becomes available.

\section{Neural Network initialization}

The initialization of an ANN can have a significant impact on the final accuracy, convergence rate, and training time, according to FL theory. It is generally accepted that the best course of action is to use the same initialization for all clients \cite{FL-original-paper}.  This major objective of this experiment is to assess this convention. To further emphasize the consequences of the initialization, the SGD optimizer with a low learning rate is employed.

\begin{table}[H]
    \center
    \makebox[0pt]{
        \begin{tabular}
            { | l | c | c | c | }
            \hline
            parameters & FL seeded init & FL random init & centralized training\\\hline
            total clients & 5 & 5 & 1\\\hline
            clients per GE & 3 & 3 & 1\\\hline
            steps per GE & 5 & 5 & examples/batch \\\hline
            batch size & 20  & 20  & 20 \\\hline
            examples per GE & 300  & 300  & all \\\hline
            GEs to use all examples & 200  & 200  & - \\\hline
        \end{tabular}
    }
    \caption[Experiment 6 parameters]{Experiment 6 parameters}
    \label{table:Experiment 6 parameters}
\end{table}

The third model is used and initialized with the Glorot initializer. The model is trained twice, once initialized with the same seed across all clients, the other using random different seeds.

\begin{figure}[H]
    \center
    \input{experiments/6.tex}
    \caption[Experiment 6 results]{Experiment 6 results}
    \label{fig:Experiment 6 results}
\end{figure}

The model with a random initialization quickly approaches and settles in a suboptimal local minimum. Both centralized training and FL with seeded initialization surpass its accuracy. This behaviour is consistent with FL theory.

\section{Learning Rate (LR) decay strategies}
Another aspect of FL worth investigating is learning rate (LR) decay strategies. The following three of them are implemented:
\begin{itemize}
    \item Decay the LR every set number of GEs. All clients have the same LR at every moment.
    \item Decay the LR of a client based on the number of participated GEs. If a subset of the clients is used every GE, some clients may have been selected more times than others and as a result they will have a lower LR.
    \item The final strategy is to reduce a client's LR each time its dataset is repeated. This is an extension of the second strategy, where instead of decaying slowly the LR every few rounds, there is a big drop every \( \displaystyle \frac{\sum_{}^{}clients\ data}{\sum_{}^{} clients\ data\ used\ per\ GE} \) rounds of training.
\end{itemize}

\begin{table}[H]
    \center
    \hspace*{-9mm} \makebox[0pt]
    {
        \begin{tabular}
            { | l | c | c | c | c | }
            \hline
            parameters & FL, no decay & FL strategy 1 & FL strategy 2 & FL strategy 3\\\hline
            total clients   &     5 &     5 &     5 &     5\\\hline
            clients per GE  &     3 &     3 &     3 &     3\\\hline
            steps per GE    &     5 &     5 &     5 &     5\\\hline
            batch size      &    20 &    20 &    20 &    20\\\hline
            initial LR      &  1e-2 &  1e-2 &  1e-2 &  1e-2\\\hline
            LR decay        &     - & 0.999 & 0.999 &\( \displaystyle \frac{ 0.999 * \sum clients\ data}{\sum_{}^{} clients\ data\ used\ per\ GE} \)\\\hline
            \makecell{ decay interval\\(x = decay period) } & - & x GEs & \makecell{ x participated\\GEs } &
            \( \displaystyle \frac{ x\ participated\ GEs*\sum clients\ data }{ \sum clients\ data\ used\ per\ GE } \)\\\hline
        \end{tabular}
    }
    \caption[Experiment 7 parameters]{Experiment 7 parameters}
    \label{table:Experiment 7 parameters}
\end{table}

Each strategy is tested three times with different decay periods. The decay period dictates how often the decay applies. E.g. the second strategy with the a decay period of three means that LR decays every three participated rounds. A FL trained model without LR decay is used as a baseline.
\medskip\medskip
\begin{figure}[H]
    \center
    \addtolength{\leftskip} {-2cm}
    \addtolength{\rightskip}{-2cm}
    \input{experiments/7a.tex}%
    \input{experiments/7b.tex}
    \caption[Experiment 7 results]{Experiment 7 results, strategies 1 and 2.}
\end{figure}%
\begin{figure}[H]\ContinuedFloat
    \center
    \input{experiments/7c.tex}
    \caption[Experiment 7 results]{Experiment 7 results, strategy 3.}
    \label{fig:Experiment 7 results}
\end{figure}

All strategies seems to to perform slightly better than the baseline, except the first one with decay period = 1. In that case, the decay is too fast and the LR degenerates in a state that cannot substantially alter the weights of the NN. The last strategy appears to be the most promising, which while outperforming the others is the most straightforward.

\section{Federated Averaging (FedAvg)}
In FedAvg, a client, when participating in a training round, uses all of its data and executes multiple SGD iterations. In the prior experiment, for a client to consume all of its data 200 GEs were necessary; whereas with FedAvg, only one GE (at most) is needed. The main objective of this experiment is to demonstrate the algorithm's compatibility with the developed FL environment.

\begin{table}[H]
    \center
    \begin{tabular}
        { | l | c | }
        \hline
        parameters & FedAvg\\\hline
        total clients   & 5\\\hline
        clients per GE  & 3\\\hline
        local epochs    & 1\\\hline
        steps per epoch & 600\\\hline
        batch size      & 20\\\hline
        initial LR      &  1e-2\\\hline
    \end{tabular}
    \caption[Experiment 8 parameters]{Experiment 8 parameters}
    \label{table:Experiment 8 parameters}
\end{table}

The LR decay needs to be corrected to account for the reduced number of decay events, thus the model is trained multiple times to identify its ideal values. The prior experiment's LR decay is utilized for the first run of training, and each additional training reduces the descent slope by half. The model is also trained without LR decay.
    
\begin{figure}[H]
    \center
    \input{experiments/8.tex}
    \caption[Experiment 8 results]{Experiment 8 results}
    \label{fig:Experiment 8 results}
\end{figure}

The maximum accuracy of this model when trained locally is 92\%. This is now regarded as the minimum baseline. In addition of showing maximum accuracy, the GE where that baseline was reached is also presented.

\begin{table}[H]
    \center
    \begin{tabular}
        { | c | c | c | }
        \hline
        LR decay & Max accuracy & 0.92 @GE\\\hline
        0.819 & 0.913 & -\\\hline
        0.909 & 0.9232 & 30\\\hline
        0.955 & 0.9245 & 32\\\hline
        0.977 & 0.9245 & 27\\\hline
        No decay & 0.9224 & 34\\\hline
    \end{tabular}
    \caption[Experiment 8 results]{Experiment 8 results}
    \label{table:Experiment 8 results}
\end{table}

In comparison to the previous experiment, FedAvg requires \(\times\)100-200 less communication and the same computation to reach the target accuracy. However, there is a hidden cost in that less averaging occurs and the rebalancing effect is diminished. This becomes quite clear when training without LR decay, where overfitting is apparent.

Considering the different LR decay values, the more conservative options appear to perform best; decaying the LR too quickly causes the ANN to set in sub-optimal minima.

\section{Client Participation and Increasing parallelism}
This experiment explores the amount of multi-client parallelism that can be exploited and its effect on training. The dataset is split between 10 clients, each one holding 6000 training examples. The third model is trained with different number of participating clients per GE. 
    
\begin{table}[H]
    \center
    \begin{tabular}{ | l | c | c | c | c | }
        \hline
        Test & A & B & C & D\\\hline
        total clients   & 10 & 10 & 10 & 10\\\hline
        clients per GE  & 1 & 3 & 5 & 10\\\hline
        local epochs    & 1 & 1 & 1 & 1\\\hline
        steps per epoch & 300 & 300 & 300 & 300\\\hline
        batch size      & 20 & 20 & 20 & 20\\\hline
        initial LR      & 1e-2 & 1e-2 & 1e-2 & 1e-2\\\hline
        LR decay        & 0.977 & 0.977 & 0.977 & 0.977\\\hline
    \end{tabular}
    \caption[Experiment 9 parameters]{Experiment 9 parameters}
\end{table}

Training with one client per epoch serves as the baseline. The relative reduction in communication is calculated for the other training runs.

\begin{figure}[H]
    \center
    \input{experiments/9.tex}
    \caption[Experiment 9 results]{Experiment 9 results}
    \label{fig:Experiment 9 results}
\end{figure}

\begin{table}[H]
\center
    \begin{tabular}{ | c | c | c | }
        \hline
        client per GE & 0.92 @GE\\\hline
        1 & 159\\\hline
        3 & 61 (2.6$\times$)\\\hline
        5 & 46 (3.4$\times$)\\\hline
        10 & 51 (3.1$\times$)\\\hline
    \end{tabular}
    \caption[Experiment 9 results]{Experiment 9 results, with relative reduction in GEs.}
\end{table}
    
Using more clients per GE substantially lowers the rounds of communication needed to achieve the target accuracy. This is consistent with others works, which show even on simulations of hundreds of clients with a small dataset each, using a bit more than half of the clients in each GE yields the best results. If all the clients are used every GE, especially when under non-IID data, the model may not converge in an acceptable solution.
    
\section{Increasing computation per client}
To further reduce communication, clients can perform more local updates per GE. This may be accomplished by increasing the number of local epochs (LE), reducing the batch size (B), or both. The third model is trained by 5 clients, with 3 of them participating each GE. LR and its decay are amortized to maintain a consistent LR at each GE, regardless of the number of local updates. The goal of this experiment is to identify the behavior of the algorithm across different sets of parameters.

\begin{table}[H]
    \center
    \begin{tabular}{ | c | c c | c c | }
        \hline
        & \multicolumn{2}{|c|}{Local epochs = 1} & \multicolumn{2}{|c|}{Local epochs = 3} \\\hline
        B & updates/GE &  0.92 @GE & updates/GE &  0.92 @GE\\\hline
        600 & 20 & 309 & 60 & -\\
        300 & 40 & 212 & 120 & 67\\
        100 & 120 & 72 & 360 & 36\\
        80 & 150 & 54 & 450 & 25\\
        40 & 300 & 31 & 900 & 13\\
        20 & 600 & 25 & 1800 & 7\\
        10 & 1200 & 18 & 3600 & 15\\\hline
    \end{tabular}
    \caption[Experiment 10 results]{Experiment 10 results}
\end{table}

According to the results of the experimental, increasing local updates directly decreases the required global updates. Unlike most works, this one concentrates on small groups of clients with large local datasets. As a result, increasing the number of local epochs produces inconsistent results due to the introduction of overfitting in the local models. Regarding the batch size, there is no cost in reducing it, providing that it is large enough to completely utilize the client's hardware parallelism.