%-----------------------------------
%	REFERENCES
%-----------------------------------
@techreport{MEC,
	author =        {Yun Chao Hu and Milan Patel and Dario Sabella and Nurit Sprecher and Valerie Young},
	title =         {Mobile Edge Computing A key technology towards 5G},
	number =        {11},
	institution =   {European Telecommunications Standards Institute},
	address =       {06921 Sophia Antipolis CEDEX, France},
	Abstract =      {Mobile Edge Computing (MEC) is a new technology which is currently being standardized in an ETSI Industry Specification Group (ISG) of the same name. Mobile Edge Computing provides an IT service environment and cloud-computing capabilities at the edge of the mobile network, within the Radio Access Network (RAN) and in close proximity to mobile subscribers. The aim is to reduce latency, ensure highly efficient network operation and service delivery, and offer an improved user experience.},
	keywords =      {Mobile Edge Computing, Multiple-Access Edge Computing, cloud-computing, edge},
	month =         {9},
	year =          {2015},
	numpages =      {16},
	url =           {https://www.etsi.org/images/files/ETSIWhitePapers/etsi_wp11_mec_a_key_technology_towards_5g.pdf},
	isbn =          {979-10-92620-08-5}
}

@article{FL-original-paper,
    doi =           {10.48550/ARXIV.1602.05629},
    url =           {https://arxiv.org/abs/1602.05629},
    author =        {McMahan,  H. Brendan and Moore,  Eider and Ramage,  Daniel and Hampson,  Seth and Arcas,  Blaise Ag\"{u}era y},
    keywords =      {Machine Learning (cs.LG),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
    title =         {Communication-Efficient Learning of Deep Networks from Decentralized Data},
    publisher =     {arXiv},
    year =          {2016},
    copyright =     {arXiv.org perpetual,  non-exclusive license}
}

@article{Xu2021,
    doi =           {10.1109/access.2021.3063291},
    url =           {https://doi.org/10.1109/access.2021.3063291},
    year =          {2021},
    publisher =     {Institute of Electrical and Electronics Engineers ({IEEE})},
    volume =        {9},
    pages =         {38457--38466},
    author =        {Wenyuan Xu and Weiwei Fang and Yi Ding and Meixia Zou and Naixue Xiong},
    title =         {Accelerating Federated Learning for {IoT} in Big Data Analytics With Pruning,  Quantization and Selective Updating},
    journal =       {{IEEE} Access}
}

@article{Wei2020,
    doi =           {10.1109/tifs.2020.2988575},
    url =           {https://doi.org/10.1109/tifs.2020.2988575},
    year =          {2020},
    publisher =     {Institute of Electrical and Electronics Engineers ({IEEE})},
    volume =        {15},
    pages =         {3454--3469},
    author =        {Kang Wei and Jun Li and Ming Ding and Chuan Ma and Howard H. Yang and Farhad Farokhi and Shi Jin and Tony Q. S. Quek and H. Vincent Poor},
    title =         {Federated Learning With Differential Privacy: Algorithms and Performance Analysis},
    journal =       {{IEEE} Transactions on Information Forensics and Security}
}

@article{Mills2020,
    doi =           {10.1109/jiot.2019.2956615},
    url =           {https://doi.org/10.1109/jiot.2019.2956615},
    year =          {2020},
    month =         {8},
    publisher =     {Institute of Electrical and Electronics Engineers ({IEEE})},
    volume =        {7},
    number =        {7},
    pages =         {5986--5994},
    author =        {Jed Mills and Jia Hu and Geyong Min},
    title =         {Communication-Efficient Federated Learning for Wireless Edge Intelligence in {IoT}},
    journal =       {{IEEE} Internet of Things Journal}
}

@inbook{russell_norvig_2003_1,
    author =        {Russell, Stuart J. and Norvig, Peter},
    booktitle =     {Artificial Intelligence: A modern approach},
    edition =       {2},
    isbn =          {0137903952; 9780137903955; 0130803022; 9780130803023},
    % libgen.li/file.php?md5=2965b1a4e080390931391b293954044a
    
    title =         {Introduction},
    pages =         {31â€“32},
    
    year =          {2003},
    
    publisher =     {Pearson Education, Inc.},
    place =         {Upper Saddle River, New Jersey}
}

@inbook{russell_norvig_2003_18,
    author =        {Russell, Stuart J. and Norvig, Peter},
    booktitle =     {Artificial Intelligence: A modern approach},
    edition =       {2},
    isbn =          {0137903952; 9780137903955; 0130803022; 9780130803023},
    % libgen.li/file.php?md5=2965b1a4e080390931391b293954044a
    
    title =         {Learning from Observations},
    pages =         {649-651},
    
    year =          {2003},
    
    publisher =     {Pearson Education, Inc.},
    place =         {Upper Saddle River, New Jersey}
}

@misc{Transfer_Learning,
    doi =           {10.48550/ARXIV.1911.02685},
    url =           {https://arxiv.org/abs/1911.02685},
    author =        {Zhuang,  Fuzhen and Qi,  Zhiyuan and Duan,  Keyu and Xi,  Dongbo and Zhu,  Yongchun and Zhu,  Hengshu and Xiong,  Hui and He,  Qing},
    keywords =      {Machine Learning (cs.LG),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
    title =         {A Comprehensive Survey on Transfer Learning},
    publisher =     {arXiv},
    year =          {2019},
    copyright =     {arXiv.org perpetual,  non-exclusive license}
}

@misc{Probabilistic_Reasoning,
    doi =           {10.48550/ARXIV.1303.5718},
    url =           {https://arxiv.org/abs/1303.5718},
    author =        {Geiger,  Dan and Heckerman,  David},
    keywords =      {Artificial Intelligence (cs.AI),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
    title =         {Advances in Probabilistic Reasoning},
    publisher =     {arXiv},
    year =          {2013},
    copyright =     {arXiv.org perpetual,  non-exclusive license}
}

@article{McCulloch1943,
    doi =           {10.1007/bf02478259},
    url =           {https://doi.org/10.1007/bf02478259},
    year =          {1943},
    month =         {12},
    publisher =     {Springer Science and Business Media {LLC}},
    volume =        {5},
    number =        {4},
    pages =         {115--133},
    author =        {Warren S. McCulloch and Walter Pitts},
    title =         {A logical calculus of the ideas immanent in nervous activity},
    journal =       {The Bulletin of Mathematical Biophysics}
}

@incollection{Alexnet,
    author =        {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    booktitle =     {Advances in Neural Information Processing Systems 25},
    editor =        {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
    keywords =      {cnn deeplearning ma-zehe neuralnet},
    pages =         {1097--1105},
    publisher =     {Curran Associates, Inc.},
    title =         {ImageNet Classification with Deep Convolutional Neural Networks},
    url =           {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
    year =          2012
}

@misc{limits_speech_recognition,
    doi =           {10.48550/ARXIV.2010.10504},
    url =           {https://arxiv.org/abs/2010.10504},
    author =        {Zhang,  Yu and Qin,  James and Park,  Daniel S. and Han,  Wei and Chiu,  Chung-Cheng and Pang,  Ruoming and Le,  Quoc V. and Wu,  Yonghui},
    keywords =      {Audio and Speech Processing (eess.AS),  Machine Learning (cs.LG),  Sound (cs.SD),  FOS: Electrical engineering,  electronic engineering,  information engineering,  FOS: Electrical engineering,  electronic engineering,  information engineering,  FOS: Computer and information sciences,  FOS: Computer and information sciences},
    title =         {Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition},
    publisher =     {arXiv},
    year =          {2020},
    copyright =     {arXiv.org perpetual,  non-exclusive license}
}

@misc{natural_language,
    doi =           {10.48550/ARXIV.1807.10854},
    url =           {https://arxiv.org/abs/1807.10854},
    author =        {Otter,  Daniel W. and Medina,  Julian R. and Kalita,  Jugal K.},
    keywords =      {Computation and Language (cs.CL),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
    title =         {A Survey of the Usages of Deep Learning in Natural Language Processing},
    publisher =     {arXiv},
    year =          {2018},
    copyright =     {arXiv.org perpetual,  non-exclusive license}
}

@article{Climatology,
    doi =           {10.1029/2019ms001705},
    url =           {https://doi.org/10.1029/2019ms001705},
    year =          {2019},
    month =         {8},
    publisher =     {American Geophysical Union ({AGU})},
    volume =        {11},
    number =        {8},
    pages =         {2680--2693},
    author =        {Jonathan A. Weyn and Dale R. Durran and Rich Caruana},
    title =         {Can Machines Learn to Predict Weather? Using Deep Learning to Predict Gridded 500-{hPa} Geopotential Height From Historical Weather Data},
    journal =       {Journal of Advances in Modeling Earth Systems}
}

@article{biotechnology,
    doi =           {10.1016/j.tibtech.2018.08.005},
    url =           {https://doi.org/10.1016/j.tibtech.2018.08.005},
    year =          {2019},
    month =         {3},
    publisher =     {Elsevier {BV}},
    volume =        {37},
    number =        {3},
    pages =         {310--324},
    author =        {Jason Riordon and Du{\v{s}}an Sovilj and Scott Sanner and David Sinton and Edmond W.K. Young},
    title =         {Deep Learning with Microfluidics for Biotechnology},
    journal =       {Trends in Biotechnology}
}

@article{dl_evolution,
    doi =           {10.1016/j.cogsys.2018.08.023},
    url =           {https://doi.org/10.1016/j.cogsys.2018.08.023},
    year =          {2018},
    month =         {12},
    publisher =     {Elsevier {BV}},
    volume =        {52},
    pages =         {701--708},
    author =        {Ritika Wason},
    title =         {Deep learning: Evolution and expansion},
    journal =       {Cognitive Systems Research}
}

@inbook{Elgendy2020_ml_feature,
    booktitle =     {Deep learning for vision systems},
    edition =       {1},
    author =        {Elgendy, Mohamed},
    isbn =          {1617296198; 9781617296192},
    % libgen.li/file.php?md5=b26784397a47df876a8a9ef6060d9819
    % https://www.manning.com/books/deep-learning-for-vision-systems
    
    title =         {Feature extraction},
    pages =         {27},
    
    month =         {12},
    year =          {2020},
    
    publisher =     {Manning Publications},
    address =       {New York, NY}
}

@InProceedings{pmlr-v9-glorot10a,
    title = 	    {Understanding the difficulty of training deep feedforward neural networks},
    author = 	    {Glorot, Xavier and Bengio, Yoshua},
    booktitle = 	{Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
    pages = 	    {249--256},
    editor = 	    {Teh, Yee Whye and Titterington, Mike},
    volume = 	    {9},
    series = 	    {Proceedings of Machine Learning Research},
    address = 	    {Chia Laguna Resort, Sardinia, Italy},
    eventdate =     {2010-05-13/2010-05-15},
    publisher =     {PMLR},
    pdf = 	        {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
    url = 	        {https://proceedings.mlr.press/v9/glorot10a.html},
    abstract = 	    {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@article{He_Init_paper,
    author =        {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    title =         {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
    journal =       {CoRR},
    volume =        {abs/1502.01852},
    year =          {2015},
    url =           {http://arxiv.org/abs/1502.01852},
    eprinttype =    {arXiv},
    eprint =        {1502.01852},
    timestamp =     {Wed, 17 Apr 2019 17:23:45 +0200},
    biburl =        {https://dblp.org/rec/journals/corr/HeZR015.bib},
    bibsource =     {dblp computer science bibliography, https://dblp.org}
}

@article{backpropagation_original,
    doi =           {10.1038/323533a0},
    url =           {https://doi.org/10.1038/323533a0},
    year =          {1986},
    month =         {10},
    publisher =     {Springer Science and Business Media {LLC}},
    volume =        {323},
    number =        {6088},
    pages =         {533--536},
    author =        {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
    title =         {Learning representations by back-propagating errors},
    journal =       {Nature}
}


@InProceedings{nesterov_momentum,
    title = 	    {On the importance of initialization and momentum in deep learning},
    author = 	    {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
    booktitle = 	{Proceedings of the 30th International Conference on Machine Learning},
    pages = 	    {1139--1147},
    year = 	        {2013},
    editor = 	    {Dasgupta, Sanjoy and McAllester, David},
    volume = 	    {28},
    number =        {3},
    series = 	    {Proceedings of Machine Learning Research},
    address = 	    {Atlanta, Georgia, USA},
    month = 	    {17--19 Jun},
    publisher =     {PMLR},
    pdf = 	        {http://proceedings.mlr.press/v28/sutskever13.pdf},
    url = 	        {https://proceedings.mlr.press/v28/sutskever13.html},
    abstract = 	    {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.}
}

@ARTICLE{FL_comprehensive_survey,
    author =        {Lim, Wei Yang Bryan and Luong, Nguyen Cong and Hoang, Dinh Thai and Jiao, Yutao and Liang, Ying-Chang and Yang, Qiang and Niyato, Dusit and Miao, Chunyan},
    journal =       {IEEE Communications Surveys \& Tutorials}, 
    title =         {Federated Learning in Mobile Edge Networks: A Comprehensive Survey}, 
    year =          {2020},
    volume =        {22},
    number =        {3},
    pages =         {2031-2063},
    doi =           {10.1109/COMST.2020.2986024}
 }
 
@misc{GBoard_FL,
    doi =           {10.48550/ARXIV.1811.03604},
    url =           {https://arxiv.org/abs/1811.03604},
    author =        {Hard,  Andrew and Rao,  Kanishka and Mathews,  Rajiv and Ramaswamy,  Swaroop and Beaufays,  Fran\c{c}oise and Augenstein,  Sean and Eichner,  Hubert and Kiddon,  ChloÃ© and Ramage,  Daniel},
    keywords =      {Computation and Language (cs.CL),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
    title =         {Federated Learning for Mobile Keyboard Prediction},
    publisher =     {arXiv},
    year =          {2018},
    copyright =     {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}


%-----------------------------------
%	EXTERNAL LINKS
%-----------------------------------

@legislation{GDPR,
    author =        {European Parliament and Council of the European Union},
    title =         {REGULATION (EU) 2016/679 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)},
    institution =   {European Union},
    journal =       {Official Journal of the European Union},
    year =          {2016},
    month =         {05},
    day =           {04},
    url =           {http://data.europa.eu/eli/reg/2016/679/oj},
    urldate = {2022-05-25}
}

@legislation{CCPA,
    author =        {Chau A. and Hertzberg S. and Dodd S.},
    title =         {The California Consumer Privacy Act of 2018},
    institution =   {California State Senate},
    year =          {2018},
    month =         {6},
    day =           {29},
    url =           {https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180AB375},
    urldate = {2022-05-25}
}

@online{activation_function,
    title =         {How to Choose an Activation Function for Deep Learning},
    author =         {Jason Brownlee},
    year =          {2021},
    month =         {1},
    day =           {22},
    url =           {https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/},
    urldate =       {2022-06-04}
}

@online{dl_history,
    title =         {History of Neural Networks},
    institution =   {Stanford University},
    url =           {https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/index.html},
    urldate =       {2022-06-01}
}

@online{IBM_Neural_Networks,
    institution =   {IBM},
    author =        {IBM Cloud Education},
    title =         {Neural Networks},
    year =          {2020},
    month =         {8},
    day =           {17},
    url =           {https://www.ibm.com/cloud/learn/neural-networks},
    urldate =       {2022-06-04}
}

@online{CS231n_stanford_cnn,
    institution =   {Stanford University},
    title =         {Convolutional Neural Networks for Visual Recognition},
    year =          {2022},
    url =           {https://cs231n.github.io/convolutional-networks/},
    urldate =       {2022-06-05}
}

@online{ML_feature,
    organization =  {Manning Publications},
    title =         {Convolutional Neural Networks for Visual Recognition},
    year =          {2022},
    url =           {https://freecontent.manning.com/the-computer-vision-pipeline-part-4-feature-extraction/},
    urldate =       {2022-06-05}
}

@online{Xavier_initialization,
    author =        {James D. McCaffrey},
    title =         {Neural Network Glorot Initialization},
    year =          {2017},
    month =         {6},
    day =           {21},
    
    url =           {https://jamesmccaffrey.wordpress.com/2017/06/21/neural-network-glorot-initialization/},
    urldate =       {2022-06-11}
}

@online{Glorot_initialization_large,
    author =        {James D. McCaffrey},
    title =         {Neural Network Glorot Initialization},
    organization =  {Visual Studio Magazine},
    
    year =          {2019},
    month =         {5},
    day =           {9},
    
    url =           {https://visualstudiomagazine.com/articles/2019/09/05/neural-network-glorot.aspx},
    urldate =       {2022-06-11}
}

@online{Glorot_He_initialization,
    author =        {Andrew Jones},
    title =         {An Explanation of Xavier Initialization},
    year =          {2015},
    month =         {2},
    day =           {14},
    
    url =           {https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization},
    urldate =       {2022-06-11}
}

@online{loss_functions,
    title =         {Loss functions},
    organization =  {Peltarion},
    
    url =           {https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions},
    urldate =       {2022-06-11}
}

@online{voronoi_cells,
    author =        {Hristo Hristov},
    title =         {An Introduction to the Voronoi Diagram},
    year =          {2021},
    month =         {11},
    day =           {21},
    
    url =           {https://www.baeldung.com/cs/voronoi-diagram},
    urldate =       {2022-06-11}
}

@online{Backpropagation_wiki,
    title =         {Backpropagation},
    organization =  {Wikipedia},
    year =          {2022},
    month =         {03},
    day =           {11},
    
    url =           {https://en.wikipedia.org/wiki/Backpropagation},
    urldate =       {2022-06-13}
}

@online{gradient_wiki,
    title =         {Gradient},
    organization =  {Wikipedia},
    year =          {2022},
    month =         {05},
    day =           {21},
    
    url =           {https://en.wikipedia.org/wiki/Gradient},
    urldate =       {2022-06-13}
}

@online{IBM_Gradient_Descent,
    title =         {Gradient Descent},
    organization =  {IBM Cloud Education},
    year =          {2020},
    month =         {10},
    day =           {27},
    
    url =           {https://www.ibm.com/cloud/learn/gradient-descent},
    urldate =       {2022-06-14}
}

@online{gradient_descent_wiki,
    title =         {Gradient descent},
    organization =  {Wikipedia},
    year =          {2022},
    month =         {06},
    day =           {06},
    
    url =           {https://en.wikipedia.org/wiki/Gradient_descent},
    urldate =       {2022-06-14}
}

@online{IBM_overfitting,
    title =         {Overfitting},
    organization =  {IBM Cloud Education},
    year =          {2021},
    month =         {03},
    day =           {03},
    
    url =           {https://www.ibm.com/cloud/learn/overfitting},
    urldate =       {2022-06-17}
}

%-----------------------------------
%	Unused references
%-----------------------------------
